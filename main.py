# -*- coding: utf-8 -*-
"""2024aa05664_Problem_Statement_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BdySi4U-_5mpaXSoDrGH5OLZDPdpY3tA

# Telecom Predicting customer churn study

#### Predicting customer churn is a critical task for telecommunications companies aiming to retain their customers and reduce revenue loss. Customer churn refers to the phenomenon where customers discontinue using the company's services. By accurately predicting which customers are likely to churn, the company can take proactive measures to improve customer satisfaction and retention.

We have decided to create a `Logistic Regression` model to solve the problem. The Logistic Regression model will be expected to output a `Churn Probability` for every data under test.

`customer_churn_dataset.csv` holds the given dataset

The dataset includes various customer demographics, account information, and service usage patterns.

- `Personal Data (gender, senior citizen, Partner, dependents)`
- `Services Related (Tenure with the provider, PhoneService, MultipleLines, InternetService, OnlineSecurity, OnlineBackup)`

### Import Libraries
"""

# Uploading the dataset for running
#from google.colab import files
#uploaded = files.upload()

import warnings, os
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as smd
from sklearn.metrics import confusion_matrix
from sklearn.tree import DecisionTreeClassifier, export_text, export_graphviz
from sklearn.ensemble import RandomForestClassifier
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, precision_score, recall_score, f1_score
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, roc_auc_score

"""### Import the Input Dataset"""

df = pd.read_csv('customer_churn_dataset.csv')
df.head()

"""# Columns of the dataframe


"""

df.columns

"""# Dimensions of the dataframe


"""

df.shape

df.info()

"""# Statistical aspects of the dataframe"""

df.describe()

"""## Exploratory Data Analysis (EDA)
The EDA process will comprise of Univariate and Multivariate Analysis alomg with abundant Visualisations.

### Box plots to check for outliers in key features
"""

#plt.hist(df['tenure'])


plt.figure(figsize=(15, 10))
plt.subplot(2, 2, 1)
sns.boxplot(x='Churn', y='tenure', data=df)
plt.title('Tenure by Churn')
plt.xlabel('Churn (0=No, 1=Yes)')
plt.ylabel('Tenure')

plt.subplot(2, 2, 2)
sns.boxplot(x='Churn', y='SeniorCitizen', data=df)
plt.title('Senior Citizen by Churn')
plt.xlabel('Churn (0=No, 1=Yes)')
plt.ylabel('Senior Citizen')

"""#### Churn Split in terms of Counts"""

# Display the counts of Churns in the entire data. This provides an indication of whether the data is unbalanced.

plt1 = sns.countplot(df['Churn'])

"""### Whether having Multiple Internet Connections have influence on Churn"""

plt.figure(figsize=(15,5))
pie_MultipleLines_Y = pd.DataFrame(df[df['MultipleLines'] == 'Yes']['Churn'].value_counts())
pie_MultipleLines_Y.plot.pie(subplots = True,  labels = pie_MultipleLines_Y.index.values, autopct='%1.1f%%', startangle= 50)
plt.title('Multiple lines of internet connectivity')
plt.gca().set_aspect('equal')

pie_MultipleLines_N = pd.DataFrame(df[df['MultipleLines'] == 'No']['Churn'].value_counts())
pie_MultipleLines_N.plot.pie(subplots = True,  labels = pie_MultipleLines_N.index.values, autopct='%1.1f%%', startangle= 45)
plt.title('Single line of internet connectivity')
plt.gca().set_aspect('equal')

pie_MultipleLines_N = pd.DataFrame(df[df['MultipleLines'] == 'No phone service']['Churn'].value_counts())
pie_MultipleLines_N.plot.pie(subplots = True,  labels = pie_MultipleLines_N.index.values, autopct='%1.1f%%', startangle= 45)
plt.title('No phone service')
plt.gca().set_aspect('equal')
plt.show()  # This will display the plot window

"""### Whether type of Internet Service has influence on Churn"""

plt.figure(figsize=(15,5))
pie_InternetService_fo = pd.DataFrame(df[df['InternetService'] == "Fiber optic"]['Churn'].value_counts())
pie_InternetService_fo.plot.pie(subplots = True, labels = pie_InternetService_fo.index.values, autopct='%1.1f%%', startangle= 75)
plt.title('Fiber Optic')
plt.gca().set_aspect('equal')

pie_InternetService_dsl = pd.DataFrame(df[df['InternetService'] == "DSL"]['Churn'].value_counts())
pie_InternetService_dsl.plot.pie(subplots = True, labels = pie_InternetService_dsl.index.values, autopct='%1.1f%%', startangle= 35)
plt.title('DSL')
plt.gca().set_aspect('equal')

pie_InternetService_no = pd.DataFrame(df[df['InternetService'] == "No"]['Churn'].value_counts())
pie_InternetService_no.plot.pie(subplots = True, labels = pie_InternetService_no.index.values, autopct='%1.1f%%', startangle= 13)
plt.title('No Internet Service')
plt.gca().set_aspect('equal')

plt.show()

"""# Calculate the correlation matrix"""

# Check the corr values of final list of variables

corr_matrix = df.select_dtypes(include=np.number).corr()
# Plot the correlation heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Correlation Matrix Heatmap')
plt.tight_layout()
plt.show()

# Correlation with target variable ('Chance of Admit ')
target_correlations = corr_matrix['Churn'].sort_values(ascending=False)
print("\nFeature correlations with Churn:")
print(target_correlations)

# Visualize feature importance based on correlation with target
plt.figure(figsize=(10, 6))
target_correlations.drop('Churn').plot(kind='bar')
plt.title('Feature Correlations with Churn')
plt.ylabel('Correlation Coefficient')
plt.xlabel('Features')
plt.tight_layout()
plt.show()

"""## Data Pre-processing
Feature        |Type                                   |  Why It Matters for Churn?
- Gender | Categorical (Binary) | Often not a strong predictor, but may be useful in combination with other factors.
- SeniorCitizen | Categorical (0 = No, 1 = Yes) | Older customers may have different loyalty patterns.
- Partner        | Categorical (Yes/No)                   | Customers with a partner might be less likely to churn.
- Dependent      | Categorical (Yes/No)                   | Customers with dependents may be more stable.
- Tenure         | Numerical (Months)                     | One of the strongest predictors – new customers tend to churn more.
- PhoneService   | Categorical (Yes/No)                   | Basic phone service alone may lead to lower retention.
- MultipleLines  | Categorical (No, Yes, No PhoneService) | More lines may indicate higher engagement, reducing churn.
- InternetService| Categorical (DSL, Fiber Optic, No)     | Fiber customers churn more (due to price), while DSL users may stay longer.
- OnlineService  | Categorical (Yes/No/No internet service)                   | Customers without online services churn more.
- OnlineBackup   | Categorical (Yes/No/No internet service)                   | Extra services increase customer stickiness.

#Check  for skewnesx and outlier
"""

def detect_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]
    return outliers

# List of features to check for outliers (excluding categorical and target variables)
features_to_check = ['SeniorCitizen', 'tenure']

print("\nOutlier Detection:")
for column in features_to_check:
    outliers = detect_outliers(df, column)
    print(f"{column}: {len(outliers)} outliers detected")

# Check for skewness in the data
print("\nSkewness of numerical features:")
for column in features_to_check + ['Churn']:
    print(f"{column}: {df[column].skew():.4f}")

#### Pre-process the Tenure
#### Replacing the numerical value with median as data is slightly skewed

df['tenure'] = df['tenure'].fillna(df['tenure'].median())
df['SeniorCitizen'] = df['SeniorCitizen'].fillna(df['SeniorCitizen'].median())

#If tenure of service is more than 60 than it that person can be considered as senior citizen
#df['SeniorCitizen'] = df.apply(lambda x: 1 if x['tenure'] > 60 else 0, axis=1)

# Interaction: tenure_SeniorInteraction = tenure * SeniorCitizen.
# Senior Citizen is considered to be more loyal customer hence creating the interaction
#df['tenure_SeniorInteraction'] = df['tenure'] * df['SeniorCitizen']

#Customer doesn't have phone service in multiple lines than Phone service can be No
df['PhoneService'] = df.apply(lambda x: 'No' if x['MultipleLines'] == 'No phone service' else x['PhoneService'], axis=1)

"""### Convert Binary Categorical Features (Yes/No) to 0/1"""

# List of variables to map

varlist =  ['PhoneService', 'Partner', 'Dependents']

# Defining the map function
def binary_map(x):
    return x.map({'Yes': 1, "No": 0})

# Applying the function to the original list
df[varlist] = df[varlist].apply(binary_map)

df.head()

"""### For Nominal Categorical Features with multiple possible values, create dummy variable (one-hot encoding)"""

# Creating a dummy variable for some of the categorical variables and dropping the first one.
dummy1 = pd.get_dummies(df[['gender', 'InternetService', 'MultipleLines', 'OnlineSecurity', 'OnlineBackup']], drop_first=True)

# Adding the results to the master dataframe
df = pd.concat([df, dummy1], axis=1)

"""### Dropping the Original Features"""

# We have created dummies for the below variables, so we can drop them
df = df.drop(columns=['gender','MultipleLines','InternetService', 'OnlineSecurity', 'OnlineBackup'])
# We have created the tenure Senior Citizen interaction
#df = df.drop(columns=['SeniorCitizen','tenure'])
df = df.drop(columns=['Partner','Dependents','customerID'])

df.head()

df.info()

# Removing NaN TotalCharges rows
df = df[~np.isnan(df['PhoneService'])]

"""Now you can see that you have all variables as numeric.

#### Checking for Missing Values and Inputing Them
"""

# Adding up the missing values (column-wise)
df.isnull().sum()

"""Now we don't have any missing values

## Test-Train Split of Input Dataset
"""



# 2️⃣ Define Features (X) and Target (y)
y = df['Churn']  # Target variable (0 or 1)
X = df.drop(columns=['Churn'])  # Features

# Splitting the data into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=100)

"""## Finding Correlations among Features"""

# Check the corr values of final list of variables
cor = df.corr()
cor

# Find out the Fields with high correlation

correlated_features = set()
for i in range(len(cor.columns)):
    for j in range(i):
        if abs(cor.iloc[i, j]) > 0.7:
            colname1 = cor.columns[i]
            colname2 = cor.columns[j]
            print(abs(cor.iloc[i, j]), "--", i, '--', j, '--', colname1, '--', colname2)
            correlated_features.add(colname1)
            correlated_features.add(colname2)
print(cor.columns)
print('------')
print(correlated_features)

"""### Correlation Matrix/Heatmap"""

X_train = pd.DataFrame(X_train, columns=X.columns)
plt.figure(figsize = (20,10))
print(type(X_train))
sns.heatmap(X_train.corr(),annot = True)
plt.show()

"""## Feature Scaling
# Standardize numerical features (Senior Citizen, Tenure)
"""

scaler = StandardScaler()
# Standardize numerical features
features_to_standardize = ['SeniorCitizen', 'tenure']

# Ensure X_train is a DataFrame before scaling
X_train = pd.DataFrame(X_train, columns=X.columns)  # Convert to DataFrame if it's not

# Fit and transform on the selected columns
X_train[features_to_standardize] = scaler.fit_transform(X_train[features_to_standardize])

"""## Logistic Regression Model


"""

# --- Logistic Regression ---
logreg = LogisticRegression()
logreg.fit(X_train, y_train)
y_pred_lg = logreg.predict(X_test)

# Create the Accuracy Score
print("Accuracy:", accuracy_score(y_test, y_pred_lg))
print("\nClassification Report:\n", classification_report(y_test, y_pred_lg))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_lg))

"""# Hyperparameter Tuning with GridSearchCV"""

param_grid = {
    'logreg__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]
}
pipeline = Pipeline([
    ('logreg', LogisticRegression(max_iter=1000))
])

grid_search = GridSearchCV(
    pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1
)
grid_search.fit(X_train, y_train)

print("\nBest Parameters:", grid_search.best_params_)
print("Best CV Score:", grid_search.best_score_)

# -----------------------------------
# 9. Evaluate the Final Model on Test Data
# -----------------------------------
best_model = grid_search.best_estimator_
y_pred_logreg = best_model.predict(X_test)
y_prob_logreg = best_model.predict_proba(X_test)[:, 1]

# Create the Accuracy Score
print("Accuracy:", accuracy_score(y_test, y_pred_logreg))
print("\nClassification Report:\n", classification_report(y_test, y_pred_logreg))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_logreg))

# Create the Confusion Matrix
cm = confusion_matrix(y_test, y_pred_logreg)
conf_matrix = pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])
plt.figure(figsize = (8,5))
sns.heatmap(conf_matrix, annot=True,fmt='d',cmap="YlGnBu")

"""### Other Classification Statistics"""

TN=cm[0,0]
TP=cm[1,1]
FN=cm[1,0]
FP=cm[0,1]
sensitivity=TP/float(TP+FN)
specificity=TN/float(TN+FP)

print('The acuuracy of the model = TP+TN/(TP+TN+FP+FN) = ',(TP+TN)/float(TP+TN+FP+FN),'\n',

'Missclassifications = 1-Accuracy = ',1-((TP+TN)/float(TP+TN+FP+FN)),'\n',

'Sensitivity/Recall or True Positive Rate = TP/(TP+FN) = ',TP/float(TP+FN),'\n',

'Specificity or True Negative Rate = TN/(TN+FP) = ',TN/float(TN+FP),'\n',

'Precision/Positive Predictive value = TP/(TP+FP) = ',TP/float(TP+FP),'\n',

'Negative predictive Value = TN/(TN+FN) = ',TN/float(TN+FN),'\n',

'Positive Likelihood Ratio = Sensitivity/(1-Specificity) = ',sensitivity/(1-specificity),'\n',

'Negative likelihood Ratio = (1-Sensitivity)/Specificity = ',(1-sensitivity)/specificity)

"""### Some Pointers from the above Classification Metrics:
- `Sensitivity/Recall`: In the example of Churn prediction, it gives us the percentage of Correctly Predicted Churns from the pool of Actual Churns.
- `Specificity`: Gives us the percentage of Correctly Predicted Non-Churns from the pool of Actual Non-Churns.
- `Precision`: Gives us the percentage of Correctly Predicted Churns from the pool of Total Predicted Churns.

## Decision Tree Model
"""

# Decision Tree with Grid Search CV
dt_params = {
    'max_depth': [None, 3, 5, 7, 10],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'criterion': ['gini', 'entropy']
}

dt_model = DecisionTreeClassifier(random_state=42)
dt_cv = GridSearchCV(dt_model, dt_params, cv=5, scoring='accuracy')
dt_cv.fit(X_train, y_train)

print("\nDecision Tree Results:")
print(f"Best parameters: {dt_cv.best_params_}")
print(f"Best cross-validation score: {dt_cv.best_score_:.4f}")

# Evaluate Decision Tree on test set
dt_best = dt_cv.best_estimator_
y_pred_dt = dt_best.predict(X_test)
y_prob_dt = best_model.predict_proba(X_test)[:, 1]

# Create the Accuracy Score
print("Accuracy:", accuracy_score(y_test, y_pred_dt))
print("\nClassification Report:\n", classification_report(y_test, y_pred_dt))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_dt))

# Create the Confusion Matrix
cm = confusion_matrix(y_test, y_pred_dt)
conf_matrix = pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])
plt.figure(figsize = (8,5))
sns.heatmap(conf_matrix, annot=True,fmt='d',cmap="YlGnBu")

# Calculate and plot ROC curves for both models
plt.figure(figsize=(10, 8))

# ROC curve for Logistic Regression
log_reg_fpr, log_reg_tpr, _ = roc_curve(y_test, y_prob_logreg)
log_reg_auc = roc_auc_score(y_test, y_prob_logreg)
plt.plot(log_reg_fpr, log_reg_tpr, label=f'Logistic Regression (AUC = {log_reg_auc:.4f})')

# ROC curve for Decision Tree
dt_fpr, dt_tpr, _ = roc_curve(y_test, y_prob_dt)
dt_auc = roc_auc_score(y_test, y_prob_dt)
plt.plot(dt_fpr, dt_tpr, label=f'Decision Tree (AUC = {dt_auc:.4f})')

# Plot the diagonal (random classifier)
plt.plot([0, 1], [0, 1], 'k--', label='Random')

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves')
plt.legend()
plt.grid(True)
plt.show()

"""## Random Forest Classifier Model"""

# --- Random Forest ---
#rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
#rf_model.fit(X_train, y_train)
#y_pred_rf = rf_model.predict(X_test)
#y_prob_rf = rf_model.predict_proba(X_test)[:, 1]

# 1. Ensemble Method: Random Forest
rf_params = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 5, 10, 15],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}

rf_model = RandomForestClassifier(random_state=42)
rf_cv = GridSearchCV(rf_model, rf_params, cv=5, scoring='accuracy')
rf_cv.fit(X_train, y_train)

print("Random Forest Results:")
print(f"Best parameters: {rf_cv.best_params_}")
print(f"Best cross-validation score: {rf_cv.best_score_:.4f}")

# Evaluate Random Forest on test set
rf_best = rf_cv.best_estimator_
y_pred_rf = rf_best.predict(X_test)
y_prob_rf = rf_best.predict_proba(X_test)[:, 1]

# Create the Accuracy Score
print("Accuracy:", accuracy_score(y_test, y_pred_rf))
print("\nClassification Report:\n", classification_report(y_test, y_pred_rf))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_rf))

## KNN Model Classifier Model

# KNN Regression
knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train, y_train)
y_pred_knn = knn_model.predict(X_test)
y_prob_knn = knn_model.predict_proba(X_test)[:, 1]



print(f"Test accuracy: {accuracy_score(y_test, rf_pred):.4f}")
print("\nClassification Report:")
print(classification_report(y_test, rf_pred))

# 2. K-Nearest Neighbors
knn_params = {
    'n_neighbors': [3, 5, 7, 9, 11, 13],
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan']
}

knn = KNeighborsClassifier()
knn_cv = GridSearchCV(knn, knn_params, cv=5, scoring='accuracy')
knn_cv.fit(X_train, y_train)

print("\nK-Nearest Neighbors Results:")
print(f"Best parameters: {knn_cv.best_params_}")
print(f"Best cross-validation score: {knn_cv.best_score_:.4f}")

# Evaluate KNN on test set
knn_best = knn_cv.best_estimator_
knn_pred = knn_best.predict(X_test)
knn_prob = knn_best.predict_proba(X_test)[:, 1]

print(f"Test accuracy: {accuracy_score(y_test, knn_pred):.4f}")
print("\nClassification Report:")
print(classification_report(y_test, knn_pred))

# Create the Accuracy Score
print("Accuracy:", accuracy_score(y_test, y_pred_knn))
print("\nClassification Report:\n", classification_report(y_test, y_pred_knn))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_knn))

# --- Evaluation ---
models = ['Logistic Regression', 'Decision Tree', 'Random Forest', 'KNN']
y_preds = [y_pred_logreg, y_pred_dt, y_pred_rf, y_pred_knn]
y_probs= [y_prob_logreg, y_prob_dt, y_prob_rf, y_prob_knn]

metrics = ['Precision', 'Recall', 'F1-Score', 'AUC-ROC']
results = []

for model, y_pred in zip(models, y_preds):
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    auc_roc = roc_auc_score(y_test, y_pred)

    results.append([model, precision, recall, f1, auc_roc])

# --- Comparison Chart ---
results_df = pd.DataFrame(results, columns=['Model', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC'])
results_df = results_df.melt(id_vars=['Model'], var_name='Metric', value_name='Score')
results_df.head()

plt.figure(figsize=(10, 6))
sns.barplot(x='Metric', y='Score', hue='Model', data=results_df)
plt.title('Model Performance Comparison')
plt.xticks(rotation=45)
plt.legend(loc='best')
plt.tight_layout()
plt.show()

"""**Best Model:** Decision Tree
Why Decision Tree is the Best?

*   Highest Accuracy (50.6%) compared to the other models.
*   Balanced Precision and Recall for both churn (1) and non-churn (0) customers. This is crucial in churn prediction, as we want to avoid being biased towards predicting one class over the other.
*   Random Forest: despite being an ensemble method, performs slightly worse, which may indicate overfitting or improper hyperparameter tuning.
*   KNN: It has lower accuracy and a less balanced performance compared to Decision Tree.
*   Logistic Regression: While it has a high recall for churners (0.99), its precision is low (0.50), meaning it misclassifies many non-churners as churners.

**Final Recommendation:**
The Decision Tree model is the best among the trained models for telecom churn prediction. However, since all models have low accuracy (~50%), further improvements are needed in dataset as the data is distributed 50% around 0 and 1 predictions
"""